{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Real‑Time Camera + Qwen2‑VL / LLaVA 1.6 (Template)\n",
        "\n",
        "This notebook is a **starter template** for building a kid‑friendly, real‑time camera assistant.\n",
        "\n",
        "## What this template does\n",
        "1. Captures frames from your webcam (OpenCV).\n",
        "2. (Optional) Runs **YOLOv8** for fast real‑time object detection and cropping.\n",
        "3. Sends a selected frame/crop to a **Vision‑Language Model** (Qwen2‑VL or LLaVA) **on demand** (e.g., when the user presses a key).\n",
        "4. Produces a short, child‑friendly answer.\n",
        "\n",
        "## Why “on demand” inference?\n",
        "Running a VLM on every frame is slow/expensive. The best UX is:\n",
        "- YOLO runs continuously (30–60 FPS)\n",
        "- VLM runs only when the child asks a question / object changes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Environment check\n",
        "If you have an NVIDIA GPU, install the appropriate PyTorch CUDA build.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install dependencies\n",
        "Choose one path:\n",
        "- **Path A (Local webcam loop):** OpenCV + (optional) YOLO + Transformers VLM\n",
        "- **Path B (Web app):** Gradio WebRTC webcam streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core\n",
        "%pip -q install opencv-python pillow numpy\n",
        "\n",
        "# VLM\n",
        "%pip -q install transformers accelerate sentencepiece\n",
        "\n",
        "# Optional: YOLOv8 for real-time detection/tracking\n",
        "%pip -q install ultralytics\n",
        "\n",
        "# Optional: If you want a browser-based webcam UI\n",
        "%pip -q install gradio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Pick your model\n",
        "### A) Qwen2‑VL (recommended)\n",
        "- Pros: strong open model family; good multimodal instruction following.\n",
        "- Suggested sizes:\n",
        "  - **2B** if you care about speed\n",
        "  - **7B** for better quality (needs a stronger GPU)\n",
        "\n",
        "### B) LLaVA 1.6\n",
        "- Also strong; similar usage patterns.\n",
        "\n",
        "⚠️ Note: model IDs can change; always check Hugging Face for the exact repo name you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========= CONFIG =========\n",
        "# Set ONE of these.\n",
        "MODEL_FAMILY = 'qwen2-vl'  # 'qwen2-vl' or 'llava'\n",
        "\n",
        "# Examples (edit as needed):\n",
        "QWEN_MODEL_ID = 'Qwen/Qwen2-VL-2B-Instruct'   # faster\n",
        "# QWEN_MODEL_ID = 'Qwen/Qwen2-VL-7B-Instruct' # higher quality\n",
        "\n",
        "# LLaVA examples (edit as needed):\n",
        "LLAVA_MODEL_ID = 'llava-hf/llava-1.6-mistral-7b-hf'\n",
        "\n",
        "# Child-friendly style\n",
        "SYSTEM_STYLE = (\n",
        "    'You are talking to a 1.5-year-old child. '\n",
        "    'Use very simple words. Use 1-2 short sentences. '\n",
        "    'Be warm and encouraging. No scary content.'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load the VLM (Transformers)\n",
        "This cell provides **template loaders**. Depending on the exact model repo, you may need to adjust the processor/model classes.\n",
        "\n",
        "If you hit a loading error, paste the error and the exact model ID you chose, and we’ll adapt it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "vlm = None\n",
        "processor = None\n",
        "\n",
        "if MODEL_FAMILY == 'qwen2-vl':\n",
        "    from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "    model_id = QWEN_MODEL_ID\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    vlm = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
        "        device_map='auto' if device == 'cuda' else None,\n",
        "    )\n",
        "elif MODEL_FAMILY == 'llava':\n",
        "    # Many LLaVA HF repos are compatible with AutoProcessor + AutoModelForVision2Seq\n",
        "    from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "    model_id = LLAVA_MODEL_ID\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    vlm = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
        "        device_map='auto' if device == 'cuda' else None,\n",
        "    )\n",
        "else:\n",
        "    raise ValueError('MODEL_FAMILY must be qwen2-vl or llava')\n",
        "\n",
        "print('Loaded:', model_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) VLM helper: ask about an image\n",
        "You can ask:\n",
        "- “What is this?”\n",
        "- “What color is it?”\n",
        "- “What is it used for?”\n",
        "\n",
        "This helper keeps answers short for kids.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_vlm(pil_image: Image.Image, user_question: str, max_new_tokens: int = 60) -> str:\n",
        "    \"\"\"Ask the loaded VLM a question about the given PIL image.\"\"\"\n",
        "    prompt = f\"{SYSTEM_STYLE}\\nQuestion: {user_question}\"\n",
        "\n",
        "    # Many vision-language models accept a chat-style format; others accept simple text.\n",
        "    # This is a safe default. If your chosen model expects a different format, adjust here.\n",
        "    inputs = processor(images=pil_image, text=prompt, return_tensors='pt')\n",
        "\n",
        "    if device == 'cuda':\n",
        "        inputs = {k: v.to('cuda') if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = vlm.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Clean up: sometimes the model echoes the prompt\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Real-time camera loop (OpenCV)\n",
        "Controls:\n",
        "- Press **Space** to ask the model about the current frame\n",
        "- Press **q** to quit\n",
        "\n",
        "Tip: You can replace `question = ...` dynamically using speech-to-text later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError('Could not open webcam. Try changing the camera index: VideoCapture(1)')\n",
        "\n",
        "print('Webcam started. Press SPACE to ask, q to quit.')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print('Failed to grab frame')\n",
        "        break\n",
        "\n",
        "    cv2.imshow('Camera', frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "\n",
        "    # Ask on demand\n",
        "    if key == 32:  # SPACE\n",
        "        # Convert BGR (OpenCV) -> RGB (PIL)\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil = Image.fromarray(rgb)\n",
        "\n",
        "        # Example questions (swap this with speech input later)\n",
        "        question = 'What is this?'\n",
        "        # question = 'What color is it?'\n",
        "        # question = 'What is it used for?'\n",
        "\n",
        "        print('\\nAsking:', question)\n",
        "        answer = ask_vlm(pil, question, max_new_tokens=60)\n",
        "        print('Answer:', answer)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Optional: YOLOv8 real-time detection + crop → VLM\n",
        "This is the recommended production pattern:\n",
        "- YOLO detects objects fast\n",
        "- When user asks, crop the most confident detection and ask VLM on that crop\n",
        "\n",
        "Controls:\n",
        "- Press **Space** to ask about the most confident detected object crop\n",
        "- Press **q** to quit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Use a lightweight model for speed\n",
        "yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError('Could not open webcam. Try changing the camera index: VideoCapture(1)')\n",
        "\n",
        "print('YOLO webcam started. Press SPACE to ask about top detection, q to quit.')\n",
        "\n",
        "last_crop = None\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # YOLO inference\n",
        "    results = yolo.predict(frame, verbose=False)\n",
        "    r = results[0]\n",
        "\n",
        "    # Draw boxes and keep the best crop\n",
        "    best = None\n",
        "    best_conf = -1\n",
        "    if r.boxes is not None and len(r.boxes) > 0:\n",
        "        for b in r.boxes:\n",
        "            x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())\n",
        "            conf = float(b.conf[0].item())\n",
        "            cls = int(b.cls[0].item())\n",
        "            label = f\"{r.names.get(cls, cls)} {conf:.2f}\"\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, label, (x1, max(0, y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
        "\n",
        "            if conf > best_conf:\n",
        "                best_conf = conf\n",
        "                best = (x1, y1, x2, y2, label)\n",
        "\n",
        "    if best is not None:\n",
        "        x1, y1, x2, y2, label = best\n",
        "        crop = frame[max(0,y1):max(0,y2), max(0,x1):max(0,x2)]\n",
        "        if crop.size > 0:\n",
        "            last_crop = (crop.copy(), label)\n",
        "\n",
        "    cv2.imshow('YOLO Camera', frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "\n",
        "    if key == 32:  # SPACE\n",
        "        if last_crop is None:\n",
        "            print('No detection to ask about yet.')\n",
        "            continue\n",
        "\n",
        "        crop_bgr, yolo_label = last_crop\n",
        "        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil = Image.fromarray(crop_rgb)\n",
        "\n",
        "        question = 'What is this?'\n",
        "        # question = 'What color is it?'\n",
        "        # question = 'What is it used for?'\n",
        "\n",
        "        print('\\nYOLO detected:', yolo_label)\n",
        "        print('Asking:', question)\n",
        "        answer = ask_vlm(pil, question, max_new_tokens=60)\n",
        "        print('Answer:', answer)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Optional: Browser webcam UI (Gradio)\n",
        "This is useful when:\n",
        "- you want to run it on a server and open it from a phone\n",
        "- you want a quick demo without dealing with OpenCV windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "def respond(image: Image.Image, question: str):\n",
        "    if image is None:\n",
        "        return 'No image received.'\n",
        "    if not question:\n",
        "        question = 'What is this?'\n",
        "    return ask_vlm(image, question, max_new_tokens=60)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=[gr.Image(type='pil', sources=['webcam']), gr.Textbox(label='Question', value='What is this?')],\n",
        "    outputs=gr.Textbox(label='Answer'),\n",
        "    title='Kid-Friendly Vision Assistant (Template)',\n",
        "    description='Use the webcam. Ask: What is this? What color is it? What is it used for?'\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next upgrades\n",
        "- Add speech-to-text (Whisper) so the child can ask questions by voice.\n",
        "- Add text-to-speech (Coqui TTS) to speak answers.\n",
        "- Add safety filters + a kid-safe response policy.\n",
        "- Add caching: if the object hasn’t changed, reuse the last answer.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}